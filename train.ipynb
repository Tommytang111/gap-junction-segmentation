{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58f8f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from utilities import UpBlock, DownBlock, DoubleConv, GenDLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import BinaryRecall, BinaryPrecision, BinaryF1Score\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "#Custom Libraries\n",
    "from resize_image import resize_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7f50733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seeds\n",
    "def seed_everything(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bed85410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make Dataset Class\n",
    "#Class can load any mask as long as the model corresponds to the mask type\n",
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, images, labels, masks=None, augmentation=None, data_size=(512, 512), train=True):\n",
    "        self.image_paths = [os.path.join(images, img) for img in os.listdir(images)]\n",
    "        self.label_paths = [os.path.join(labels, lbl) for lbl in os.listdir(labels)]\n",
    "        self.mask_paths = [os.path.join(masks, mask) for mask in os.listdir(masks)] if masks else None\n",
    "        self.augmentation = augmentation\n",
    "        self.data_size = data_size\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        #Apply resizing with padding if image is not expected size\n",
    "        raw_image = cv2.imread(self.image_paths[idx], cv2.IMREAD_COLOR)\n",
    "\n",
    "        if (raw_image.shape[0] != self.data_size[0]) or (raw_image.shape[1] != self.data_size[1]): \n",
    "            raw_image = resize_image(raw_image, self.data_size[0], self.data_size[1], (0,0,0))\n",
    "        \n",
    "        #Read image, label, and mask\n",
    "        image = cv2.cvtColor(np.array(raw_image), cv2.COLOR_BGR2GRAY)\n",
    "        label = cv2.imread(self.label_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE) if self.mask_paths else None\n",
    "\n",
    "        #Convert mask/label to binary for model classification\n",
    "        label[label > 0] = 1\n",
    "        if mask is not None:\n",
    "            mask[mask > 0] = 1\n",
    "        \n",
    "        #Add augmentation clause later\n",
    "        #Add entity recognition clause later if needed\n",
    "        \n",
    "        #Convert to tensors\n",
    "        image = ToTensor()(image).float()\n",
    "        label = torch.from_numpy(label).long()\n",
    "        if mask is not None:\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "\n",
    "        return image, label, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "854dd359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(512, 512, 3)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = TrainingDataset(\n",
    "    images=\"/home/tommytang111/data/sem_adult/SEM_split/s250-259/imgs\",\n",
    "    labels=\"/home/tommytang111/data/sem_adult/SEM_split/s250-259/gts\",\n",
    "    train=True\n",
    ")\n",
    "images=\"/home/tommytang111/data/sem_adult/SEM_split/s250-259/imgs\"\n",
    "a = [os.path.join(images, img) for img in os.listdir(images) if img.lower().endswith('.png')]\n",
    "b= cv2.imread(a[0], cv2.IMREAD_COLOR)\n",
    "data_size = (512, 512)\n",
    "\n",
    "if (b.shape[-1] != data_size[0]) or (b.shape[-2] != data_size[1]):\n",
    "    print('hi')\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a57491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize and load datasets\n",
    "train = TrainingDataset(\n",
    "    images=\"/home/tommytang111/data/sem_adult/SEM_split/s250-259/imgs\",\n",
    "    labels=\"/home/tommytang111/data/sem_adult/SEM_split/s250-259/gts\",\n",
    "    train=True\n",
    ")\n",
    "\n",
    "valid = TrainingDataset(\n",
    "    images=\"/home/tommytang111/data/sem_adult/SEM_split/s200-209/imgs\",\n",
    "    labels=\"/home/tommytang111/data/sem_adult/SEM_split/s200-209/gts\",\n",
    "    train=False\n",
    ")   \n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=8, shuffle=True, num_workers=8, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid, batch_size=8, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2fa57f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize model and send to gpu\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"UNet Architecture\"\"\"\n",
    "    def __init__(self, out_classes=2, up_sample_mode='conv_transpose', three=False, attend=False, residual=False, scale=False, spatial=False, dropout=0, classes=2):\n",
    "        \"\"\"Initialize the UNet model\"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        self.three = three\n",
    "        self.up_sample_mode = up_sample_mode\n",
    "        self.dropout=dropout\n",
    "\n",
    "        # Downsampling Path\n",
    "        self.down_conv1 = DownBlock(1, 64, three=three, spatial=False, residual=residual) # 3 input channels --> 64 output channels\n",
    "        self.down_conv2 = DownBlock(64, 128, three=three, spatial=spatial, dropout=self.dropout, residual=residual) # 64 input channels --> 128 output channels\n",
    "        self.down_conv3 = DownBlock(128, 256, spatial=spatial, dropout=self.dropout, residual=residual) # 128 input channels --> 256 output channels\n",
    "        self.down_conv4 = DownBlock(256, 512, spatial=spatial, dropout=self.dropout, residual=residual) # 256 input channels --> 512 output channels\n",
    "        # Bottleneck\n",
    "        self.double_conv = DoubleConv(512, 1024,spatial=spatial, dropout=self.dropout, residual=residual)\n",
    "        # Upsampling Path\n",
    "        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode, dropout=self.dropout, residual=residual) # 512 + 1024 input channels --> 512 output channels\n",
    "        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode, dropout=self.dropout, residual=residual)\n",
    "        self.up_conv2 = UpBlock(128+ 256, 128, self.up_sample_mode, dropout=self.dropout, residual=residual)\n",
    "        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n",
    "        # Final Convolution\n",
    "        self.conv_last = nn.Conv2d(64, 1 if classes == 2 else classes, kernel_size=1)\n",
    "        self.attend = attend\n",
    "        if scale:\n",
    "            self.s1, self.s2 = torch.nn.Parameter(torch.ones(1), requires_grad=True), torch.nn.Parameter(torch.ones(1), requires_grad=True) # learn scaling\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the UNet model\n",
    "        x: (16, 1, 512, 512)\n",
    "        \"\"\"\n",
    "        # print(x.shape)\n",
    "        x, skip1_out = self.down_conv1(x) # x: (16, 64, 256, 256), skip1_out: (16, 64, 512, 512) (batch_size, channels, height, width)    \n",
    "        x, skip2_out = self.down_conv2(x) # x: (16, 128, 128, 128), skip2_out: (16, 128, 256, 256)\n",
    "        if self.three: x = x.squeeze(-3)   \n",
    "        x, skip3_out = self.down_conv3(x) # x: (16, 256, 64, 64), skip3_out: (16, 256, 128, 128)\n",
    "        x, skip4_out = self.down_conv4(x) # x: (16, 512, 32, 32), skip4_out: (16, 512, 64, 64)\n",
    "        x = self.double_conv(x) # x: (16, 1024, 32, 32)\n",
    "        x = self.up_conv4(x, skip4_out) # x: (16, 512, 64, 64)\n",
    "        x = self.up_conv3(x, skip3_out) # x: (16, 256, 128, 128)\n",
    "        if self.three: \n",
    "            #attention_mode???\n",
    "            skip1_out = torch.mean(skip1_out, dim=2)\n",
    "            skip2_out = torch.mean(skip2_out, dim=2)\n",
    "        x = self.up_conv2(x, skip2_out) # x: (16, 128, 256, 256)\n",
    "        x = self.up_conv1(x, skip1_out) # x: (16, 64, 512, 512)\n",
    "        x = self.conv_last(x) # x: (16, 1, 512, 512)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\")    \n",
    "model = UNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf895300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize loss function and optimizer\n",
    "loss_fn = GenDLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64375e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send evaluation metrics to device\n",
    "recall = BinaryRecall().to(device)\n",
    "precision = BinaryPrecision().to(device)\n",
    "f1 = BinaryF1Score().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c4bf6673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training function\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    #Reset metrics for each epoch\n",
    "    recall.reset()\n",
    "    precision.reset()\n",
    "    f1.reset()\n",
    "    \n",
    "    for batch, (X, y, _) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Calculate metrics after converting predictions to binary\n",
    "        pred_binary = torch.sigmoid(pred) > 0.5\n",
    "        \n",
    "        #Update metrics\n",
    "        recall.update(pred_binary, y)\n",
    "        precision.update(pred_binary, y)\n",
    "        f1.update(pred_binary, y)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    #Compute final metrics per epoch\n",
    "    train_recall = recall.compute().item()\n",
    "    train_precision = precision.compute().item()\n",
    "    train_f1 = f1.compute().item()\n",
    "    train_loss_per_epoch = train_loss / num_batches \n",
    "    \n",
    "    return train_loss_per_epoch, train_recall.item(), train_precision.item(), train_f1.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0f1ff8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define validation function\n",
    "def validate(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    #Reset metrics for each epoch\n",
    "    recall.reset()\n",
    "    precision.reset()\n",
    "    f1.reset()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y, _ in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "        #Calculate metrics\n",
    "        pred_binary = torch.sigmoid(pred) > 0.5\n",
    "        \n",
    "        #Update metrics\n",
    "        recall.update(pred_binary, y)\n",
    "        precision.update(pred_binary, y)\n",
    "        f1.update(pred_binary, y)\n",
    "            \n",
    "        #Compute final metrics per epoch\n",
    "        val_recall = recall.compute().item()\n",
    "        val_precision = precision.compute().item()\n",
    "        val_f1 = f1.compute().item()\n",
    "        val_loss_per_epoch = test_loss / num_batches\n",
    "\n",
    "    return val_loss_per_epoch, val_recall.item(), val_precision.item(), val_f1.item()\n",
    "    print(f\"Avg loss: {test_loss:>7f}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [06:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_13856/1957242918.py\", line 25, in __getitem__\n    image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2GRAY)\ncv2.error: OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#Training\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m train_loss, train_recall, train_precision, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#Validation\u001b[39;00m\n\u001b[1;32m     14\u001b[0m val_loss, val_recall, val_precision, val_f1 \u001b[38;5;241m=\u001b[39m validate(valid_dataloader, model, loss_fn)\n",
      "Cell \u001b[0;32mIn[66], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m precision\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     10\u001b[0m f1\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     13\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/gap_junction/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/gap_junction/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1515\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1513\u001b[0m worker_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(idx)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gap_junction/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1550\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1550\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/gap_junction/lib/python3.10/site-packages/torch/_utils.py:750\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31merror\u001b[0m: Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_13856/1957242918.py\", line 25, in __getitem__\n    image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2GRAY)\ncv2.error: OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-21:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/threading.py\", line 1096, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "Process Process-20:\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/threading.py\", line 1096, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/tommytang111/.conda/envs/gap_junction/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "epochs = 100\n",
    "best_f1 = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    progress_bar = tqdm(total=len(train_dataloader), position=0, leave=True)\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    \n",
    "    #Training\n",
    "    train_loss, train_recall, train_precision, train_f1 = train(train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    #Validation\n",
    "    val_loss, val_recall, val_precision, val_f1 = validate(valid_dataloader, model, loss_fn)\n",
    "\n",
    "    #Update learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    #Print metrics\n",
    "    print(f\"Train | Loss: {train_loss:.4f}, Recall: {train_recall:.4f}, Precision: {train_precision:.4f}, F1: {train_f1:.4f}\")\n",
    "    print(f\"Val   | Loss: {val_loss:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}, F1: {val_f1:.4f}\")\n",
    "    \n",
    "    #Log best model state based on F1 score\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_epoch = epoch\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71062d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the best logged model state\n",
    "torch.save(best_model_state, \"/home/tommytang111/gap-junction-segmentationmodels/unet_v1.pt\")\n",
    "print(\"Saved PyTorch Model to unet_v1.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gap_junction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
