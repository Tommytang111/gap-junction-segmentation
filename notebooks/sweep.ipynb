{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96043e2a",
   "metadata": {},
   "source": [
    "#### Wandb Hyperparameter (Agent) Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "068e301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import sys\n",
    "# Add parent directory to sys.path\n",
    "parent_dir = '/home/tommytang111/gap-junction-segmentation/code/src'\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from utilities import UpBlock, DownBlock, DoubleConv, GenDLoss, FocalLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import BinaryRecall, BinaryPrecision, BinaryF1Score\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm #Change to tqdm.tqdm if not using Jupyter Notebook\n",
    "import copy\n",
    "import wandb\n",
    "#Custom Libraries\n",
    "from resize_image import resize_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f350a1fd",
   "metadata": {},
   "source": [
    "#### Set Reproducible Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55fe71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def worker_init_fn(worker_id):\n",
    "    seed = 42 + worker_id\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de409952",
   "metadata": {},
   "source": [
    "#### Define Augmentation Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08b07506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom augmentation\n",
    "def get_custom_augmentation():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Affine(scale=(0.9,1.1), rotate=10, translate_percent=0.15, shear = (-5, 5), p=0.9),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.2, p=0.5),\n",
    "        A.GaussNoise(p=0.3),\n",
    "        A.Normalize(mean=0.0, std=1.0),\n",
    "        A.Resize(512, 512),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "def get_custom_augmentation2():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Affine(scale=(0.8,1.2), rotate=360, translate_percent=0.15, shear=(-45, 45), p=0.9),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.GaussNoise(p=0.3),\n",
    "        A.Normalize(mean=0.0, std=1.0),\n",
    "        A.Resize(512, 512),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "# Light augmentation for gap junction segmentation\n",
    "def get_light_augmentation():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.Transpose(p=0.5),\n",
    "        A.GaussNoise(p=0.3),\n",
    "        A.Blur(blur_limit=3, p=0.2),\n",
    "        A.Normalize(mean=0.0, std=1.0),  # For grayscale\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "# Medium augmentation\n",
    "def get_medium_augmentation():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.Transpose(p=0.5),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.1, \n",
    "            scale_limit=0.2, \n",
    "            rotate_limit=15, \n",
    "            border_mode=cv2.BORDER_CONSTANT, \n",
    "            value=0, \n",
    "            p=0.5\n",
    "        ),\n",
    "        A.ElasticTransform(\n",
    "            alpha=1, \n",
    "            sigma=50, \n",
    "            alpha_affine=50, \n",
    "            border_mode=cv2.BORDER_CONSTANT, \n",
    "            value=0, \n",
    "            p=0.3\n",
    "        ),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "        A.Blur(blur_limit=3, p=0.2),\n",
    "        A.CLAHE(clip_limit=2.0, p=0.3),\n",
    "        A.Normalize(mean=0.0, std=1.0),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "# Heavy augmentation\n",
    "def get_heavy_augmentation():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.Transpose(p=0.5),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.15, \n",
    "            scale_limit=0.3, \n",
    "            rotate_limit=25, \n",
    "            border_mode=cv2.BORDER_CONSTANT, \n",
    "            value=0, \n",
    "            p=0.6\n",
    "        ),\n",
    "        A.ElasticTransform(\n",
    "            alpha=1, \n",
    "            sigma=50, \n",
    "            alpha_affine=50, \n",
    "            border_mode=cv2.BORDER_CONSTANT, \n",
    "            value=0, \n",
    "            p=0.4\n",
    "        ),\n",
    "        A.GridDistortion(p=0.3),\n",
    "        A.OpticalDistortion(p=0.3),\n",
    "        A.GaussNoise(var_limit=(10.0, 80.0), p=0.4),\n",
    "        A.OneOf([\n",
    "            A.Blur(blur_limit=3),\n",
    "            A.GaussianBlur(blur_limit=3),\n",
    "            A.MedianBlur(blur_limit=3),\n",
    "        ], p=0.3),\n",
    "        A.CLAHE(clip_limit=2.0, p=0.4),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
    "        A.Normalize(mean=0.0, std=1.0),\n",
    "        ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab624b",
   "metadata": {},
   "source": [
    "#### Make Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "287cb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class can load any mask as long as the model corresponds to the mask type\n",
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, images, labels, masks=None, augmentation=None, data_size=(512, 512), train=True):\n",
    "        self.image_paths = sorted([os.path.join(images, img) for img in os.listdir(images)])\n",
    "        self.label_paths = sorted([os.path.join(labels, lbl) for lbl in os.listdir(labels)])\n",
    "        self.mask_paths = sorted([os.path.join(masks, mask) for mask in os.listdir(masks)]) if masks else None\n",
    "        self.augmentation = augmentation\n",
    "        self.data_size = data_size\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Read image, label, and mask\n",
    "        image = cv2.imread(self.image_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        label = cv2.imread(self.label_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE) if self.mask_paths else None\n",
    "        \n",
    "        #Apply resizing with padding if image is not expected size and then convert back to ndarray\n",
    "        if (image.shape[0] != self.data_size[0]) or (image.shape[1] != self.data_size[1]): \n",
    "            image = np.array(resize_image(image, self.data_size[0], self.data_size[1], (0,0,0)))\n",
    "            label = np.array(resize_image(label, self.data_size[0], self.data_size[1], (0,0,0)))\n",
    "            if mask is not None:\n",
    "                mask = np.array(resize_image(mask, self.data_size[0], self.data_size[1], (0,0,0)))\n",
    "\n",
    "        #Convert mask/label to binary for model classification\n",
    "        label[label > 0] = 1\n",
    "        if mask is not None:\n",
    "            mask[mask > 0] = 1\n",
    "        \n",
    "        #Apply augmentation if provided\n",
    "        if self.augmentation and self.train:\n",
    "            if mask is not None:\n",
    "                #Use mask in augmentation\n",
    "                augmented = self.augmentation(image=image, mask=label, label=mask)\n",
    "                image = augmented['image']\n",
    "                label = augmented['mask']\n",
    "                mask = augmented['label']\n",
    "            else:\n",
    "                #Without mask\n",
    "                augmented = self.augmentation(image=image, mask=label)\n",
    "                image = augmented['image']\n",
    "                label = augmented['mask']\n",
    "\n",
    "        #Add entity recognition clause later if needed\n",
    "        \n",
    "        # Convert to tensors if not already converted from augmentation\n",
    "        if not torch.is_tensor(image):\n",
    "            image = ToTensor()(image).float()\n",
    "        if not torch.is_tensor(label):\n",
    "            label = torch.from_numpy(label).long()\n",
    "        if mask is not None and not torch.is_tensor(mask):\n",
    "            mask = torch.from_numpy(mask).long()\n",
    "        elif mask is None:\n",
    "            mask = torch.zeros_like(label)\n",
    "\n",
    "        return image, label, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792129f0",
   "metadata": {},
   "source": [
    "#### Set Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0185c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For training with augmentation\n",
    "train_augmentation = get_custom_augmentation()  # Change to get_medium_augmentation() or get_heavy_augmentation() as needed\n",
    "\n",
    "# For validation without augmentation\n",
    "valid_augmentation = A.Compose([\n",
    "    A.Normalize(mean=0.0, std=1.0),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227f47e",
   "metadata": {},
   "source": [
    "#### Initialize and Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6134f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TrainingDataset(\n",
    "    images=\"/home/tommytang111/gap-junction-segmentation/data/sem_adult/SEM_split/s250-259/imgs\",\n",
    "    labels=\"/home/tommytang111/gap-junction-segmentation/data/sem_adult/SEM_split/s250-259/gts\",\n",
    "    augmentation=train_augmentation,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "valid = TrainingDataset(\n",
    "    images=\"/home/tommytang111/gap-junction-segmentation/data/sem_adult/SEM_split/s200-209/imgs\",\n",
    "    labels=\"/home/tommytang111/gap-junction-segmentation/data/sem_adult/SEM_split/s200-209/gts\",\n",
    "    augmentation=valid_augmentation,\n",
    "    train=False\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=8, shuffle=True, num_workers=4, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "valid_dataloader = DataLoader(valid, batch_size=8, shuffle=False, num_workers=4, pin_memory=True, worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab54b7",
   "metadata": {},
   "source": [
    "#### Initialize model and send to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5b99f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"UNet Architecture\"\"\"\n",
    "    def __init__(self, out_classes=2, up_sample_mode='conv_transpose', three=False, attend=False, residual=False, scale=False, spatial=False, dropout=0, classes=2):\n",
    "        \"\"\"Initialize the UNet model\"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        self.three = three\n",
    "        self.up_sample_mode = up_sample_mode\n",
    "        self.dropout=dropout\n",
    "\n",
    "        # Downsampling Path\n",
    "        self.down_conv1 = DownBlock(1, 64, three=three, spatial=False, residual=residual) # 3 input channels --> 64 output channels\n",
    "        self.down_conv2 = DownBlock(64, 128, three=three, spatial=spatial, dropout=self.dropout, residual=residual) # 64 input channels --> 128 output channels\n",
    "        self.down_conv3 = DownBlock(128, 256, spatial=spatial, dropout=self.dropout, residual=residual) # 128 input channels --> 256 output channels\n",
    "        self.down_conv4 = DownBlock(256, 512, spatial=spatial, dropout=self.dropout, residual=residual) # 256 input channels --> 512 output channels\n",
    "        # Bottleneck\n",
    "        self.double_conv = DoubleConv(512, 1024,spatial=spatial, dropout=self.dropout, residual=residual)\n",
    "        # Upsampling Path\n",
    "        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode, dropout=self.dropout, residual=residual) # 512 + 1024 input channels --> 512 output channels\n",
    "        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode, dropout=self.dropout, residual=residual)\n",
    "        self.up_conv2 = UpBlock(128+ 256, 128, self.up_sample_mode, dropout=self.dropout, residual=residual)\n",
    "        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n",
    "        # Final Convolution\n",
    "        self.conv_last = nn.Conv2d(64, 1 if classes == 2 else classes, kernel_size=1)\n",
    "        self.attend = attend\n",
    "        if scale:\n",
    "            self.s1, self.s2 = torch.nn.Parameter(torch.ones(1), requires_grad=True), torch.nn.Parameter(torch.ones(1), requires_grad=True) # learn scaling\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the UNet model\n",
    "        x: (16, 1, 512, 512)\n",
    "        \"\"\"\n",
    "        # print(x.shape)\n",
    "        x, skip1_out = self.down_conv1(x) # x: (16, 64, 256, 256), skip1_out: (16, 64, 512, 512) (batch_size, channels, height, width)    \n",
    "        x, skip2_out = self.down_conv2(x) # x: (16, 128, 128, 128), skip2_out: (16, 128, 256, 256)\n",
    "        if self.three: x = x.squeeze(-3)   \n",
    "        x, skip3_out = self.down_conv3(x) # x: (16, 256, 64, 64), skip3_out: (16, 256, 128, 128)\n",
    "        x, skip4_out = self.down_conv4(x) # x: (16, 512, 32, 32), skip4_out: (16, 512, 64, 64)\n",
    "        x = self.double_conv(x) # x: (16, 1024, 32, 32)\n",
    "        x = self.up_conv4(x, skip4_out) # x: (16, 512, 64, 64)\n",
    "        x = self.up_conv3(x, skip3_out) # x: (16, 256, 128, 128)\n",
    "        if self.three: \n",
    "            #attention_mode???\n",
    "            skip1_out = torch.mean(skip1_out, dim=2)\n",
    "            skip2_out = torch.mean(skip2_out, dim=2)\n",
    "        x = self.up_conv2(x, skip2_out) # x: (16, 128, 256, 256)\n",
    "        x = self.up_conv1(x, skip1_out) # x: (16, 64, 512, 512)\n",
    "        x = self.conv_last(x) # x: (16, 1, 512, 512)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\")    \n",
    "model = UNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754bd04a",
   "metadata": {},
   "source": [
    "#### Initialize loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83fc8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = GenDLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da4916e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send evaluation metrics to device\n",
    "recall = BinaryRecall().to(device)\n",
    "precision = BinaryPrecision().to(device)\n",
    "f1 = BinaryF1Score().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9384064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, recall, precision, f1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    # Reset metrics for each epoch\n",
    "    recall.reset()\n",
    "    precision.reset()\n",
    "    f1.reset()\n",
    "    \n",
    "    for batch, (X, y, _) in tqdm(enumerate(dataloader), total=num_batches, desc=\"Training\", leave=False):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Special handling for BCEWithLogitsLoss\n",
    "        if y.dim() == 3:\n",
    "            y = y.unsqueeze(1).float()\n",
    "        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate metrics after converting predictions to binary\n",
    "        pred_binary = (torch.sigmoid(pred) > 0.5).squeeze(1)\n",
    "        \n",
    "        # Update metrics\n",
    "        if y.dim() == 4 and y.size(1) == 1:\n",
    "            y = y.squeeze(1)  # [B, 1, H, W] -> [B, H, W]\n",
    "        recall.update(pred_binary, y)\n",
    "        precision.update(pred_binary, y)\n",
    "        f1.update(pred_binary, y)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Compute final metrics per epoch\n",
    "    train_recall = recall.compute().item()\n",
    "    train_precision = precision.compute().item()\n",
    "    train_f1 = f1.compute().item()\n",
    "    train_loss_per_epoch = train_loss / num_batches \n",
    "    \n",
    "    return train_loss_per_epoch, train_recall, train_precision, train_f1\n",
    "\n",
    "def validate(dataloader, model, loss_fn, recall, precision, f1):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    # Reset metrics for each epoch\n",
    "    recall.reset()\n",
    "    precision.reset()\n",
    "    f1.reset()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y, _ in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            #Special handling for BCEWithLogitsLoss\n",
    "            if y.dim() == 3:\n",
    "                y = y.unsqueeze(1).float()\n",
    "            \n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            pred_binary = (torch.sigmoid(pred) > 0.5).squeeze(1)\n",
    "            \n",
    "            # Update metrics\n",
    "            if y.dim() == 4 and y.size(1) == 1:\n",
    "                y = y.squeeze(1)  # [B, 1, H, W] -> [B, H, W]\n",
    "            recall.update(pred_binary, y)\n",
    "            precision.update(pred_binary, y)\n",
    "            f1.update(pred_binary, y)\n",
    "            \n",
    "    # Compute final metrics per epoch\n",
    "    val_recall = recall.compute().item()\n",
    "    val_precision = precision.compute().item()\n",
    "    val_f1 = f1.compute().item()\n",
    "    val_loss_per_epoch = test_loss / num_batches\n",
    "\n",
    "    return val_loss_per_epoch, val_recall, val_precision, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c164024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep():\n",
    "    # Initialize wandb run\n",
    "    wandb.login(key=\"04e003d2c64e518f8033ab016c7a0036545c05f5\")\n",
    "    wandb.init(\n",
    "        project=\"gap-junction-segmentation\",\n",
    "        entity=\"zhen_lab\",\n",
    "        dir=\"/home/tommytang111/gap-junction-segmentation/wandb\"\n",
    "    )\n",
    "    \n",
    "    # Get hyperparameters from wandb config\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Set seeds\n",
    "    seed_everything(42)\n",
    "    \n",
    "    # Get augmentation strategy from config, default to 'medium'\n",
    "    aug_strategy = config.get('augmentation', 'custom2')\n",
    "\n",
    "    if aug_strategy == 'custom1':\n",
    "        train_aug = get_custom_augmentation()\n",
    "    elif aug_strategy == 'custom2':\n",
    "        train_aug = get_custom_augmentation2()\n",
    "\n",
    "    valid_aug = A.Compose([A.Normalize(mean=0.0, std=1.0), ToTensorV2()])\n",
    "\n",
    "    # Initialize datasets with config batch size\n",
    "    train_dataset = TrainingDataset(\n",
    "        images=\"/home/tommytang111/gap-junction-segmentation/data/pilot1/train/imgs\",\n",
    "        labels=\"/home/tommytang111/gap-junction-segmentation/data/pilot1/train/gts\",\n",
    "        augmentation=train_aug,\n",
    "        train=True\n",
    "    )\n",
    "    \n",
    "    valid_dataset = TrainingDataset(\n",
    "        images=\"/home/tommytang111/gap-junction-segmentation/data/pilot1/val/imgs\",\n",
    "        labels=\"/home/tommytang111/gap-junction-segmentation/data/pilot1/val/gts\",\n",
    "        augmentation=valid_aug,\n",
    "        train=False\n",
    "    )\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config.batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4, \n",
    "        pin_memory=True, \n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size=config.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=4, \n",
    "        pin_memory=True, \n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "    \n",
    "    # Initialize model with config dropout\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = UNet(dropout=config.dropout).to(device)\n",
    "    \n",
    "    #Loss function mapping\n",
    "    if config.loss_function == \"GenDLoss\":\n",
    "        loss_fn = GenDLoss()\n",
    "    #elif config.loss_function == \"BCEWithLogitsLoss\":\n",
    "        #loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([2.0], device=device))\n",
    "    elif config.loss_function == \"FocalLoss\":\n",
    "        loss_fn = FocalLoss(alpha=torch.Tensor([0.08, 0.92]), device=device)\n",
    "\n",
    "    #Optimizer mapping\n",
    "    if config.optimizer == \"AdamW\":\n",
    "        optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=1e-4)\n",
    "    elif config.optimizer == \"SGD\":\n",
    "        optimizer = SGD(model.parameters(), lr=config.learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "    \n",
    "    # Initialize learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=10, \n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Initialize metrics\n",
    "    recall = BinaryRecall().to(device)\n",
    "    precision = BinaryPrecision().to(device)\n",
    "    f1 = BinaryF1Score().to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    torch.cuda.empty_cache()\n",
    "    epochs = 50  # Reduced for sweep\n",
    "    best_f1 = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        # Training\n",
    "        train_loss, train_recall, train_precision, train_f1 = train(\n",
    "            train_dataloader, model, loss_fn, optimizer, recall, precision, f1\n",
    "        )\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_recall, val_precision, val_f1 = validate(\n",
    "            valid_dataloader, model, loss_fn, recall, precision, f1\n",
    "        )\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train | Loss: {train_loss:.4f}, Recall: {train_recall:.4f}, Precision: {train_precision:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"Val   | Loss: {val_loss:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}, F1: {val_f1:.4f}\")\n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "        # Log best model state\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_epoch = epoch\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            # Save best model for this run\n",
    "            model_path = f\"/home/tommytang111/gap-junction-segmentation/models/sweep_model_{wandb.run.id}.pt\"\n",
    "            \n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_recall\": train_recall,\n",
    "            \"train_precision\": train_precision,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_recall\": val_recall,\n",
    "            \"val_precision\": val_precision,\n",
    "            \"val_f1\": val_f1,\n",
    "            \"best_val_f1\": best_f1,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"best_epoch\": best_epoch,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"]\n",
    "        })\n",
    "\n",
    "    print(\"Training Complete!\")\n",
    "    torch.save(best_model_state, model_path)\n",
    "    print(\"Saved PyTorch Model to \", model_path)\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3011fb79",
   "metadata": {},
   "source": [
    "#### **Sweep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb0f2040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # or 'random', 'bayes'\n",
    "    'metric': {\n",
    "        'name': 'val_f1',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [0.01, 0.001, 0.0001]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [8]\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['AdamW']\n",
    "        },\n",
    "        'loss_function': {\n",
    "            'values': ['GenDLoss']\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0, 0.1]\n",
    "        },\n",
    "        'augmentation': {\n",
    "            'values': ['custom1', 'custom2']\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3542c082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: nsu0z7xy\n",
      "Sweep URL: https://wandb.ai/zhen_lab/gap-junction-segmentation/sweeps/nsu0z7xy\n",
      "Sweep ID: nsu0z7xy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dxfg093q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation: custom1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: GenDLoss\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: AdamW\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/tommytang111/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'gap-junction-segmentation' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'zhen_lab' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tommytang111/gap-junction-segmentation/wandb/wandb/run-20250630_151554-dxfg093q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zhen_lab/gap-junction-segmentation/runs/dxfg093q' target=\"_blank\">lunar-sweep-1</a></strong> to <a href='https://wandb.ai/zhen_lab/gap-junction-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/zhen_lab/gap-junction-segmentation/sweeps/nsu0z7xy' target=\"_blank\">https://wandb.ai/zhen_lab/gap-junction-segmentation/sweeps/nsu0z7xy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zhen_lab/gap-junction-segmentation' target=\"_blank\">https://wandb.ai/zhen_lab/gap-junction-segmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/zhen_lab/gap-junction-segmentation/sweeps/nsu0z7xy' target=\"_blank\">https://wandb.ai/zhen_lab/gap-junction-segmentation/sweeps/nsu0z7xy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zhen_lab/gap-junction-segmentation/runs/dxfg093q' target=\"_blank\">https://wandb.ai/zhen_lab/gap-junction-segmentation/runs/dxfg093q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86828cc3c53d4cf8a669dc33ccf7c7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd85c49278994acfa6b0e50a88f0cd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | Loss: 0.9614, Recall: 0.7559, Precision: 0.0196, F1: 0.0383\n",
      "Val   | Loss: 0.8702, Recall: 0.3660, Precision: 0.1152, F1: 0.1753\n",
      "-----------------------------\n",
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e94cc1632946a78f0a4d2b2ac8c693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7fd75b184490>> (for post_run_cell), with arguments args (<ExecutionResult object at 7fd78b62e230, execution_count=34 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7fd78b62fee0, raw_cell=\"#Initialize sweep\n",
      "sweep_id = wandb.sweep(sweep_con..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://wsl%2Bubuntu/home/tommytang111/gap-junction-segmentation/code/notebooks/sweep.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/gap_junction/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:614\u001b[0m, in \u001b[0;36m_WandbInit._post_run_cell_hook\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 614\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gap_junction/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:779\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    778\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 779\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gap_junction/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:293\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gap_junction/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:39\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gap_junction/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:174\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrequest_id \u001b[38;5;241m=\u001b[39m record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mmailbox_slot\n\u001b[1;32m    173\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gap_junction/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gap_junction/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gap_junction/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "#Initialize sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"gap-junction-segmentation\")\n",
    "print(f\"Sweep ID: {sweep_id}\")\n",
    "\n",
    "# Start the sweep agent\n",
    "wandb.agent(sweep_id=sweep_id, function=sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3554f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gap_junction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
